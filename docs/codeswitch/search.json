[
  {
    "objectID": "codeswitch_preprocessing_itemlevel.html",
    "href": "codeswitch_preprocessing_itemlevel.html",
    "title": "Codeswtich preprocessing pipeline",
    "section": "",
    "text": "import os\nimport mne\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport pandas as pd\n\n\nfrom scipy import signal\nfrom scipy.io import wavfile\n\n# from pybv import write_brainvision\nfrom pyprep.prep_pipeline import PrepPipeline\nfrom mne_icalabel import label_components",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "codeswitch_preprocessing_itemlevel.html#parameters",
    "href": "codeswitch_preprocessing_itemlevel.html#parameters",
    "title": "Codeswtich preprocessing pipeline",
    "section": "2.1 parameters",
    "text": "2.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/data/'\noutput_dir = os.getcwd() + '/preprocessed/1_trigger_lag_corrected/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# subjects to exclude\nexclude_subs = [\n]\n\n# trigger searching window (actual trigger time based on audio - trigger time in the data)\nt_left = -0.01\nt_right = 0.5\n\nHere we fix trigger lag fix and recode the event for item-level analysis.\n\n#### create dictionaries for item-level codes and descriptions ####\ndf = pd.read_csv(\"mapping_file.txt\", delimiter='\\t')\nmapping_file2code = dict(zip(df['filename'], df['item_code']))\nmapping_code2description = dict(zip(df['item_code'], df['description']))\n#############################################################\n\n# get list of file names\nall_files = os.listdir(input_dir)\n\n# for each file\nfor file in all_files:\n    if file.endswith('.vhdr') and (file.split('.')[0] not in exclude_subs) and (file.split('.')[0]+ '_corr.fif' not in os.listdir(output_dir)):\n\n        # read in vhdr files\n        raw = mne.io.read_raw_brainvision(input_dir + file, preload = True)\n\n        # extract sampling rate\n        eeg_sfreq = raw.info['sfreq']\n\n        #### get trigger code, audio data, and audio length ####\n        \n        # initialize dictionaries\n        trigger_dict = {} # marker: [description list]\n        audio = {} # filename: data\n        lengths = {} # filename: audio length\n\n        # read in the mapping file\n        with open('mapping_file.txt','r') as f:\n            # skip the first line (header)\n            next(f)\n            for line in f:\n                # read in the current line\n                line = line.replace('\\n','')\n                # get info\n                filename, marker, description, item_code = line.split('\\t')\n        \n                # initialize a filename list for each trigger code\n                if marker not in trigger_dict.keys():\n                    trigger_dict[marker] = []\n                # add the filename to the list\n                trigger_dict[marker].append(filename)\n\n                # get audio data for each file\n                if filename not in audio.keys():\n                \n                    # get sample rate and data of the audio file\n                    sampleRate, data = wavfile.read('codeswitch_mystim/stimuli/{}'.format(filename))\n            \n                    # the sound file is stereo, so take only 1 channel's data\n                    data = data[:,0]\n            \n                    # calculate sound file length\n                    lengths[filename] = len(data)/sampleRate\n            \n                    # reduce the sampling rate of the audio file by the factor of int(sampleRate/eeg_sfreq)\n                    data_downsampled = signal.decimate(data, int(sampleRate/eeg_sfreq), ftype='fir')\n                    \n                    # add audio data the audio dictionary\n                    audio[filename] = data_downsampled\n        ####################################################\n\n\n        #### get events ####\n        # for each stimulus, mark the block info\n        events_from_annot, event_dict = mne.events_from_annotations(raw, verbose='WARNING')\n        # only events with trigger code 1-4 are useful\n        events_from_annot = events_from_annot[events_from_annot[:, 2] &lt;= 4]\n        ######################\n        \n        \n        #### cross correction to find the audio file and correct lag correction ####\n        \n        # initialize\n        delays = np.array([]) # a delay list\n        bad_stim = [] # a bad stim list\n        corr_results = [] # list of each event's max cross-correlation coefficient\n        filename_results = [] # list of each event's filename\n        \n        # loop over each event\n        for i in range(len(events_from_annot)):\n\n            # get current event\n            event = events_from_annot[i]\n            # get the onset latency\n            time = event[0]/eeg_sfreq\n            # get the marker\n            marker = str(event[2])\n\n            # initialize dictonary of each file and its max correlation coefficent\n            singleFile_maxCorr_dict = {}\n            # initialize dictionary for each file and its lag corresponding to the max correlation coefficent\n            singleFile_maxCorrLag_dict = {}\n\n            #### find the audio file for the current event, recode the marker, and record the lag info ####\n            for name in trigger_dict[marker]:\n                \n                # get the data from the sound channel\n                audio_eeg = raw.get_data(\n                    picks = ['StimTrak'],\n                    tmin = time + t_left,\n                    tmax = time + lengths[name] + t_right,\n                )[0]\n                \n                # get actual stimulus data\n                audio_stim = audio[name]\n                \n                # z-score normalization (subtract mean, divide by std)\n                audio_eeg = (audio_eeg - np.mean(audio_eeg)) / np.std(audio_eeg)\n                audio_stim = (audio_stim - np.mean(audio_stim)) / np.std(audio_stim)\n            \n                # cross-correlation\n                corr = signal.correlate(audio_eeg, audio_stim, mode='full')\n                # normalize for signal duration\n                corr = corr / (np.linalg.norm(audio_eeg) * np.linalg.norm(audio_stim))\n                # find peak correlation value\n                singleFile_maxCorr_dict[name] = np.max(corr)\n\n                # get lags for cross-correlation\n                lags = signal.correlation_lags(\n                    audio_eeg.size,\n                    audio_stim.size,\n                    mode=\"full\")\n                # find the lag for peak correlation\n                singleFile_maxCorrLag_dict[name] = lags[np.argmax(corr)] + t_left*eeg_sfreq\n\n            # get the file giving max correlation\n            max_file = max(singleFile_maxCorr_dict, key=singleFile_maxCorr_dict.get)\n            # get the maximum correlation among all files\n            max_corr = singleFile_maxCorr_dict[max_file]\n            # get the lag\n            lag = singleFile_maxCorrLag_dict[max_file]\n            # add item-level trigger code\n            events_from_annot[i][2] = mapping_file2code[max_file]\n                \n            # if the maximum correction is less than a threshold\n            if round(max_corr,1) &lt; 0.5:\n                # mark the stim bad\n                bad_stim.append(i)\n\n            # add the maximum correlation info for the current event\n            corr_results.append(max_corr)\n            filename_results.append(max_file)\n            delays = np.append(delays,lag)\n            ##################################################\n\n        #### plot the stimtrak eeg and the audio data of the event with the minimum correlation of the current file ####\n\n        # get min corr\n        min_corr = np.argmin(corr_results)\n        # get current event info\n        event = events_from_annot[min_corr]\n        # get the onset latency\n        time = event[0]/eeg_sfreq\n        # get the file name of the event\n        name = filename_results[min_corr]\n        # get the stimtrak data\n        audio_eeg = raw.get_data(\n            picks = ['StimTrak'],\n            tmin = time + t_left,\n            tmax = time + lengths[name] + t_right,\n        )[0]\n        # actual stimulus data\n        audio_stim = audio[name]\n        # z-score normalization (subtract mean, divide by std)\n        audio_eeg = (audio_eeg - np.mean(audio_eeg)) / np.std(audio_eeg)\n        audio_stim = (audio_stim - np.mean(audio_stim)) / np.std(audio_stim)\n        # plot\n        fig, ax = plt.subplots()\n        ax.plot(audio_eeg, label = 'StimTrak', alpha = 0.6)\n        ax.plot(audio_stim, label = 'wave', alpha = 0.6)\n        ax.set_title(file)\n        ax.legend()\n        fig.savefig(output_dir + file.split('.')[0] + \"_minCor.png\", dpi=300, bbox_inches='tight')\n        ##########################\n                \n        # record number of bad stims of the current file\n        if len(bad_stim)&gt;0:\n            # wave the number of bad stims to a file\n            with open(output_dir + 'bad_stim.txt', 'a+') as f:\n                _ =f.write(file + '\\t' + str(len(bad_stim)) + ' bad stims' + '\\n')\n\n        \n        # remove events of bad stims\n        events_from_annot = np.delete(events_from_annot, bad_stim, 0)\n        \n        # remove lags of bad stims\n        delays = np.delete(delays, bad_stim, 0)\n\n        # correct for trigger lag\n        events_from_annot[:,0] = events_from_annot[:,0] + delays       \n        \n        # create item-level annotations\n        annot_from_events = mne.annotations_from_events(\n            events = events_from_annot,\n            event_desc = mapping_code2description, # item-level mapping\n            sfreq = eeg_sfreq\n        )\n        \n        # set annotations\n        raw.set_annotations(annot_from_events)\n        \n        # drop the audio channel in data\n        raw.drop_channels(['StimTrak'])\n\n        # save single-trial delay file\n        np.savetxt(output_dir + file.replace('.vhdr', '_delays.txt'), delays, fmt='%i')\n        \n        # save as a file-into-file data\n        raw.save(output_dir + file.split('.')[0]+ '_corr.fif')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "codeswitch_preprocessing_itemlevel.html#parameters-1",
    "href": "codeswitch_preprocessing_itemlevel.html#parameters-1",
    "title": "Codeswtich preprocessing pipeline",
    "section": "3.1 parameters",
    "text": "3.1 parameters\n\n#### parameters ####\n\n# set directory\ninput_dir = os.getcwd() + '/preprocessed/1_trigger_lag_corrected/'\noutput_dir = os.getcwd() + '/preprocessed/2_bad_channel_corrected/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# filter cutoff frequencies (low/high)\nf_low = 1\nf_high = 100\n\n# resampling frequency\nf_res = 250\n\n# line frequency\nline_freq = 60\n\n# preprocessing parameters\nprep_params = {\n    \"ref_chs\": 'eeg',\n    \"reref_chs\": 'eeg', # average re-reference\n    \"line_freqs\": np.arange(line_freq, f_res/2, line_freq),\n}\n\n# create a montage file for the pipeline\nmontage = mne.channels.make_standard_montage(\"standard_1020\")\n\n# interpolation method\n# method=dict(eeg=\"spline\")\n\n\n#####################################################\n#### Preprocessing (filtering, resampling, bad channel detection/interpoloation, re-reference) ####\n#####################################################\n\n# get all file namesin the folder\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n# for each file\nfor file in all_input:\n    if file.endswith(\"corr.fif\") and (file.split('.')[0]+ '_prep.fif' not in all_output):\n        \n        # read in file\n        raw = mne.io.read_raw_fif(input_dir + file, preload=True)\n\n        # set channel type for EOG channels\n        raw.set_channel_types({'Fp1':'eog', 'Fp2':'eog'})\n\n        # filter\n        raw.filter(l_freq = f_low, h_freq = f_high)\n        \n        #### cut off the beginning and ending part ####b\n        \n        # get the onset of the first and the last event ####\n        events_from_annot, event_dict = mne.events_from_annotations(raw, verbose='WARNING')\n\n        #### crop the file to cut off the first the last 10s portion which maybe noisy ####\n        # define the beginning time (in seconds)\n        crop_start = events_from_annot[0][0]/raw.info['sfreq'] - 10\n\n        # define the ending time (in seconds)\n        crop_end = events_from_annot[-1][0]/raw.info['sfreq'] + 10\n\n        # crop the data\n        raw.crop(\n            tmin=max(crop_start, raw.times[0]), \n            tmax=min(crop_end, raw.times[-1])\n        )\n        ####################################################################################\n        \n        # resample\n        raw.resample(sfreq = f_res)\n\n        # read in channel location info\n        raw.set_montage(montage)\n        \n        ####  Use PrePipeline to preprocess ####\n        '''\n        1. detect and interpolate bad channels\n        2. remove line noise\n        3. re-reference\n        '''\n\n        # apply pyprep\n        prep = PrepPipeline(raw, prep_params, montage, random_state=42)\n        prep.fit()\n        \n        # export a txt file for the interpolated channel info\n        with open(output_dir + 'bad_channel.txt', 'a+') as f:\n            _ =f.write(\n                file + ':\\n' +\n                \"- Bad channels original: {}\".format(prep.noisy_channels_original[\"bad_all\"]) + '\\n' +\n                \"- Bad channels after robust average reference: {}\".format(prep.interpolated_channels) + '\\n' +\n                \"- Bad channels after interpolation: {}\".format(prep.still_noisy_channels) + '\\n'\n            )\n\n        # save the pypred preprocessed data into the raw data structure\n        raw = prep.raw\n\n        # add back the reference channel\n        raw = mne.add_reference_channels(raw,'TP9')\n\n        # add the channel loc info (for the newly added reference channel)\n        raw.set_montage(montage)\n        \n        # save\n        raw.save(output_dir + file.split('.')[0]+ '_prep.fif')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "codeswitch_preprocessing_itemlevel.html#parameters-2",
    "href": "codeswitch_preprocessing_itemlevel.html#parameters-2",
    "title": "Codeswtich preprocessing pipeline",
    "section": "4.1 parameters",
    "text": "4.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/preprocessed/2_bad_channel_corrected/'\noutput_dir = os.getcwd() + '/preprocessed/3_ica/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# up to which IC you want to consider\nic_upto = 15\n# ic_upto = 99\n\n\n# get all file names in the folder\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n# initialize a dictionary for files \nfor file in all_input:\n    if file.endswith(\"prep.fif\") and (file.split('.')[0] + '_ica.fif' not in all_output): \n\n        # read in file\n        raw = mne.io.read_raw_fif(input_dir + file, preload=True)\n        \n        # make a filtered file copy ICA. It works better on signals with 1 Hz high-pass filtered and 100 Hz low-pass filtered\n        raw_filt = raw.copy().filter(l_freq = 1, h_freq = 100)\n    \n        # apply a common average referencing, to comply with the ICLabel requirements\n        raw_filt.set_eeg_reference(\"average\")\n        \n        # initialize ica parameters\n        ica = mne.preprocessing.ICA(\n            # n_components=0.999999,\n            max_iter='auto', # n-1\n            # use ‘extended infomax’ method for fitting the ICA, to comply with the ICLabel requirements\n            method = 'infomax', \n            fit_params = dict(extended=True),\n            random_state = 42,\n        )\n    \n        #### get ica solution ####\n        ica.fit(raw_filt, picks = ['eeg'])\n\n        # save ica solutions\n        ica.save(output_dir + file.split('.')[0]+ '_icaSolution.fif')\n        \n        #### ICLabel ####\n        ic_labels = label_components(raw_filt, ica, method=\"iclabel\")\n\n        # save\n        with open(output_dir + file.split('.')[0]+ '_icLabels.pickle', 'wb') as f:\n            pickle.dump(ic_labels, f)\n        \n        #### auto select brain AND other ####\n        labels = ic_labels[\"labels\"]\n        exclude_idx = [\n            idx for idx, label in enumerate(labels) if idx&lt;ic_upto and label not in [\"brain\", \"other\"]\n        ]\n    \n        # ica.apply() changes the Raw object in-place\n        ica.apply(raw, exclude=exclude_idx)\n    \n        # record the bad ICs in bad_ICs.txt\n        with open(output_dir + '/bad_ICs.txt', 'a+') as f:\n            _ = f.write(file + '\\t' + str(exclude_idx) + '\\n')\n    \n        # save data after ICA\n        raw.save(output_dir + file.split('.')[0]+ '_ica.fif')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "codeswitch_preprocessing_itemlevel.html#parameters-3",
    "href": "codeswitch_preprocessing_itemlevel.html#parameters-3",
    "title": "Codeswtich preprocessing pipeline",
    "section": "5.1 parameters",
    "text": "5.1 parameters\n\n#### parameters ####\n\n# directory\ninput_dir = os.getcwd() + '/preprocessed/3_ica/'\noutput_dir = os.getcwd() + '/preprocessed/4_erp_epochs/' # for ERP \n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# reject data\nreject_subs = [\n]\n\n# Epoch window: \nerp_t_start = -0.2; erp_t_end = 0.8\nbaseline = (-0.2, 0)\n\n# criteria to reject epoch\n# reject_criteria = dict(eeg = 100e-6)       # 100 µV\n# reject_criteria = dict(eeg = 150e-6)       # 150 µV\nreject_criteria = dict(eeg=200e-6)       # 200 µV\n\n\n# epochs for ERP\n# initialize a list for subjects with too many bad trials\ntoo_many_bad_trial_subjects = []\n\n# get file names\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n\n#### re-reference, and epoch ####\nfor file in all_input:\n    \n    if file.endswith(\"ica.fif\") and (file.split('.')[0] + '_epo.fif' not in all_output):\n\n        # skip the rejected subject\n        if file.split('_')[1] in reject_subs:\n            continue\n        \n        # read in data\n        raw = mne.io.read_raw_fif(input_dir + file, preload = True)\n        \n        # average-mastoids re-reference\n        raw.set_eeg_reference(ref_channels = ['TP9', 'TP10'])\n        \n        #### this is for source calculation ####\n        # filter the data, optional\n        # raw = raw.filter(l_freq=None, h_freq=30) \n\n        # sphere = mne.make_sphere_model('auto', 'auto', raw.info)\n        # src = mne.setup_volume_source_space(sphere=sphere, exclude=30., pos=15.)\n        # forward = mne.make_forward_solution(raw.info, trans=None, src=src, bem=sphere)\n        # raw = raw.set_eeg_reference('REST', forward=forward)\n        ########################################\n\n        \n        # get event info for segmentation\n        events_from_annot, event_dict = mne.events_from_annotations(raw, verbose='WARNING')\n        \n        # segmentation for ERP\n        epochs = mne.Epochs(\n            raw,\n            events = events_from_annot, event_id = event_dict,\n            tmin = erp_t_start, tmax = erp_t_end,\n            # apply baseline correction\n            baseline = baseline,\n            # remove epochs that meet the rejection criteria\n            reject = reject_criteria,\n            preload = True,\n        )\n\n\n        # for each event, remove 0 trial events, record info, and check if a subject is bad\n        for k, v in event_dict.items():\n            \n            # good trial count\n            trial_count = len(epochs[k])\n            \n            # remove 0 trial event\n            if trial_count==0:\n                del epochs.event_id[k]\n                \n            # good trial rate\n            goodTrial_rate = round( trial_count/sum(events_from_annot[:,2]==v), 2 )\n            \n            # record epoch summary\n            with open(output_dir + 'epoch_summary.txt', 'a+') as f:\n                _ =f.write(file.split('_')[1] + '\\t' + k + '\\t' + str(trial_count) + '\\t' + str(goodTrial_rate) + '\\n')\n\n            # mark a subject bad if any condition has fewer than 1/2 trials\n            if ( goodTrial_rate &lt; 0.5 ):\n                # mark the subject file as bad\n                if file.split('_')[1] not in too_many_bad_trial_subjects:\n                    too_many_bad_trial_subjects.append(file.split('_')[1])\n\n        \n        # save single subject file\n        epochs.save(output_dir + file.split('.')[0] + '_epo.fif',\n                   overwrite=True)\n\n\n# export the record of bad subjects for ERP\nwith open(output_dir + 'too_many_bad_trial_subjects.txt', 'w') as file:\n    # Write each item in the list to the file\n    for item in too_many_bad_trial_subjects:\n        file.write(item + '\\n')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "codeswitch_preprocessing_itemlevel.html#parameters-4",
    "href": "codeswitch_preprocessing_itemlevel.html#parameters-4",
    "title": "Codeswtich preprocessing pipeline",
    "section": "6.1 parameters",
    "text": "6.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/preprocessed/4_erp_epochs/'\noutput_dir = os.getcwd() + '/preprocessed/5_averaged/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n\n#### get ERP ####\n\n# get file names\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n# initialize a dictionary to store data\nall_evokeds = {}\n\n# bad subjects with 0 good trials in any condition\nbad_subs = [\n]\n\n# for each file\nfor file in all_input:\n    \n    if file.endswith(\"_epo.fif\"):\n        \n        # extract subject number\n        subject = file.split('_')[1]\n        \n        # skip rejected subjects\n        if subject in bad_subs:\n            continue\n        \n        # read in data\n        epochs = mne.read_epochs(input_dir + file, preload = True)\n        \n        # average | get ERP for each condition\n        evoked = epochs.average(by_event_type=True)\n\n        # initialize dictionary for single-subject ERP\n        all_evokeds[file.split('_')[1]] = {}\n\n        # add key for each condition for analysis\n        for cond in evoked:\n            # append the evoked data to the dictioncary of evoked data\n            all_evokeds[file.split('_')[1]][cond.comment] = cond\n\n# Saving the ERP data:\nwith open(output_dir + '/all_evokeds.pkl', 'wb') as f:\n    pickle.dump(all_evokeds, f)\n# del all_evokeds",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "codeswitch_preprocessing_itemlevel.html#parameters-5",
    "href": "codeswitch_preprocessing_itemlevel.html#parameters-5",
    "title": "Codeswtich preprocessing pipeline",
    "section": "7.1 parameters",
    "text": "7.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/preprocessed/5_averaged/'\n\n# participants to exclude\nexclude_ppts = []",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "codeswitch_preprocessing_itemlevel.html#single-participant-single-condition-butterfly",
    "href": "codeswitch_preprocessing_itemlevel.html#single-participant-single-condition-butterfly",
    "title": "Codeswtich preprocessing pipeline",
    "section": "7.2 single-participant, single-condition butterfly",
    "text": "7.2 single-participant, single-condition butterfly\n\n# read in the ERP data:\nwith open(input_dir + '/all_evokeds.pkl', 'rb') as file:  # Python 3: open(..., 'rb')\n    all_evokeds = pickle.load(file)\n\n# get data\nevoked = all_evokeds['836']['local_noswitch_lunchbox']\n\n# waveform\nevoked.plot()\n\n# scalp topography\ntimes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\nevoked.plot_topomap(times=times, colorbar=True)\n\nplt.show()",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "codeswitch_preprocessing_itemlevel.html#topographical-subplots",
    "href": "codeswitch_preprocessing_itemlevel.html#topographical-subplots",
    "title": "Codeswtich preprocessing pipeline",
    "section": "7.3 topographical subplots",
    "text": "7.3 topographical subplots\n\n# read in the ERP data:\nwith open(input_dir + '/all_evokeds.pkl', 'rb') as file:\n    all_evokeds = pickle.load(file)\n\n# get the list of all participants that came this far\nall_ppts = list(all_evokeds.keys())\n\n# get participants that meet criteria\nsub_ppts = []\nfor ppt in all_ppts:\n    # if it is not in the bad subject list # \n    if ppt not in exclude_ppts:\n        # append that subject to the list\n        sub_ppts.append(ppt)\n\n\n\n# extract ERPs for each condition\nlocal_switched = []\nlocal_noswitch = []\nmando_switched = []\nmando_noswitch = []\n\n# for each participant\nfor ppt in sub_ppts:\n\n    # extract item labels\n    items = all_evokeds[ppt].keys()\n\n    for cond in ['local_switched', 'local_noswitch', 'mando_switched', 'mando_noswitch']:\n        # get condition list\n        cond_list = [ x for x in items if x.rsplit('_', 1)[0]==cond ]\n\n        # compute erp\n        tmp= mne.combine_evoked([all_evokeds[ppt][x] for x in cond_list],\n                                     weights='equal')\n        # append erp to list\n        eval(cond).append(tmp)\n\n# add erp data to dictionary for plotting\nevokeds = {}\nfor cond in ['local_switched', 'local_noswitch', 'mando_switched', 'mando_noswitch']:\n    evokeds[cond] = eval(cond)\n\n\n\n################################\n#### Topographical subplots ####\n\n# figure title for the waveform\nwaveform_title = '(n = ' + str(len(sub_ppts)) + ')'\n\n# waveforms across scalp topo\n# NOTE: I don't know how to save these plots using the code\nfig = mne.viz.plot_compare_evokeds(\n    evokeds,\n    axes='topo',\n    # picks=pick_chans,\n    # combine=\"mean\",\n    show_sensors=True,\n    # colors=colors,\n    title = waveform_title,\n    # ylim=dict(eeg=[-5, 5]),\n    time_unit=\"ms\",\n    show=False,\n);\n##############################",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  }
]