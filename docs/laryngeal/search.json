[
  {
    "objectID": "laryngeal_analysis_gam.html",
    "href": "laryngeal_analysis_gam.html",
    "title": "Laryngeal analysis (gam)",
    "section": "",
    "text": "1 Notes\n\nThe analysis goal is to extract GAM measures and correlate GAM-based measures with behavioral data.\nThe model to fit each individual’s data is: bam(uV ~ s(time, condition, bs = ‘fs’, m = 1) + s(time, stim, bs = ‘fs’, m = 1), data = df, discrete = TRUE)\n\ncondition is the interaction term of poa (dorsal vs. glottal), direction (high_to_low vs. low_to_high) x stim_role (standard vs. deviant)\n\n\n\n\n2 Libraries\n\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(itsadug)\n\n\n\n3 Load data and cleaning\n\n# F0 data\nraw &lt;- read_csv(\"~/OneDrive - University of Toronto/Projects/Laryngeal/data/gam_F0.csv\")\n# data cleaning and organization\ndf_f0 &lt;- raw %&gt;%\n  pivot_longer(cols = 4:ncol(raw), names_to = \"time\", values_to = \"uV\") %&gt;%\n  separate(col = condition, into = c(\"poa\", \"blc_stan\", \"blc_devi\", \"stim_role\", \"item\")) %&gt;%\n  unite(col = \"block\", c(\"blc_stan\", \"blc_devi\"), sep = \"_\") %&gt;%\n  mutate(group = as.factor(group),\n         participant = as.factor(participant),\n         poa = as.factor(poa),\n         block = as.factor(block),\n         stim_role = as.factor(stim_role),\n         item = as.factor(item),\n         time = as.numeric(time)) %&gt;%\n  mutate(\n    immn_direction = as.factor(case_when(\n      block==\"highStan_lowDevi\" & stim_role==\"devi\" ~ \"high_to_low\",\n      block==\"lowStan_highDevi\" & stim_role==\"stan\" ~ \"high_to_low\",\n      block==\"lowStan_highDevi\" & stim_role==\"devi\" ~ \"low_to_high\",\n      block==\"highStan_lowDevi\" & stim_role==\"stan\" ~ \"low_to_high\",\n    ))\n  ) %&gt;%\n  unite(col = \"condition\", c(\"poa\", \"immn_direction\", \"stim_role\"), sep = \"/\", remove = FALSE) %&gt;%\n  mutate(condition = as.factor(condition)) %&gt;%\n  droplevels()\n\n\n# VOT data\nraw &lt;- read_csv(\"~/OneDrive - University of Toronto/Projects/Laryngeal/data/gam_vot.csv\")\n# data cleaning and organization\ndf_vot &lt;- raw %&gt;%\n  pivot_longer(cols = 4:ncol(raw), names_to = \"time\", values_to = \"uV\") %&gt;%\n  separate(col = condition, into = c(\"poa\", \"blc_stan\", \"blc_devi\", \"stim_role\", \"item\")) %&gt;%\n  unite(col = \"block\", c(\"blc_stan\", \"blc_devi\"), sep = \"_\") %&gt;%\n  mutate(group = as.factor(group),\n         participant = as.factor(participant),\n         poa = as.factor(poa),\n         block = as.factor(block),\n         stim_role = as.factor(stim_role),\n         item = as.factor(item),\n         time = as.numeric(time)) %&gt;%\n  mutate(\n    immn_direction = as.factor(case_when(\n      block==\"highStan_lowDevi\" & stim_role==\"devi\" ~ \"long_to_short\",\n      block==\"lowStan_highDevi\" & stim_role==\"stan\" ~ \"long_to_short\",\n      block==\"lowStan_highDevi\" & stim_role==\"devi\" ~ \"short_to_long\",\n      block==\"highStan_lowDevi\" & stim_role==\"stan\" ~ \"short_to_long\",\n    ))\n  ) %&gt;%\n  unite(col = \"condition\", c(\"poa\", \"immn_direction\", \"stim_role\"), sep = \"/\", remove = FALSE) %&gt;%\n  mutate(condition = as.factor(condition)) %&gt;%\n  droplevels()\n\nThe dataset consists of the following columns:\n- group = participant group (vot vs. f0)\n- participant = participant number\n- time = time in milliseconds (ranges from -200 to 800, in 4 ms bins)\n- poa = place of articulation (dorsal vs. glottal)\n- stim_role = stimulus role (standard vs. deviant)\n- uV = ERP amplitude in microvoltages\n- immn_direction = direction for computing iMMN\n\n\n4 GAM modeling and GAM-based individual measures\nWe extract the following GAM-based individual measures:\n\ntrad_erp: average amplitude of observed data in specified time window\nmodel_area: Modelled Area = geometric area (amplitude * time) under the GAM curve. This measures the area under the peak (or maximum if there is no peak); only looking for positive area’s (or negative areas)\npeak_height: Height Modelled Peak = height of the peak of the GAM smooth, or the highest point if no peak\nNMP: Normalized Modelled Peak = a measure of robustness of the peak in units of SDs. I.e., how reliably does this subject show the peak? If value is above 1, then the 95% confidence bands do not overlap, and we can be certain the peak is there. If value is between 0 and 1, then there is a lot of variation between the items.\nhalf_area_latency: Modelled Area Median Latency = fractional area latency, i.e. latency at 50% of the area (midpoint)\nmodel_peak_time: Modelled Peak Latency = latency of the modelled peak\n\nAdditionally we compute the following measures, for extra information:\n\ntrad_norm_erp: normalized traditional average\nhasPeak: TRUE/FALSE; is there a peak in the modeled signal in the search window?\ngam_erp: average of the GAM smooth in the time window\n\nFor all these measures holds that we look at a specified time window or search window. The traditional measure requires a narrower time window (e.g. 150-300 ms post stimulus onset), the GAM measures require a search window which can be wider (here we use 0 to 800 ms post stimulus onset). We only look for negative peaks.\nThe next section of code runs the GAMs for one single participant, extracts the measures and creates the plots (in a separate pdf document).\nThe model to fit each individual’s data is:\nbam(uV ~ s(time, condition, bs = ‘fs’, m = 1) + s(time, stim, bs = ‘fs’, m = 1), discrete = TRUE)\n\n# define search window \nsearch_min = 0; search_max = 800;\n# define classic erp window (this can be from permutation test)\ntrad_min &lt;- 150; trad_max &lt;- 300\n\n# loop over groups\nfor (group in c(\"f0\", \"vot\")) {\n  \n  # get data\n  df &lt;- get(paste0(\"df\", \"_\", group))\n  # define poa levels\n  poas &lt;- levels(df$poa)\n  # define direction levels\n  directions &lt;- levels(df$immn_direction)\n  \n  # initialize the full dataframe\n  df_gam &lt;- data.frame()\n  \n  # get participant list\n  participants &lt;- levels(df$participant)\n  \n  # loop over participants for modeling\n  for (participant in participants) {\n    \n    # get single participant data\n    df_tmp &lt;- df %&gt;%\n      filter(participant == participant) %&gt;%\n      droplevels()\n    \n    # modeling\n    model &lt;- bam(uV ~ \n                   # poa * stim_role +        # fixed effects\n                   s(time, by = condition) +  # smooth for each poa x direction x stim_role\n                   s(time, item, bs = \"fs\", m = 1), # random smooth by item\n                 data = df_tmp, \n                 discrete = TRUE)  # for large data for speed\n    \n    # summary(model)\n    \n    # %%%%% extract peak height, peak time, and NMP %%%%%%\n    \n    # get values and SE for every individual time point\n    min_time &lt;- min(model$model[, \"time\"])\n    max_time &lt;- max(model$model[, \"time\"])\n    nval = length(seq(min_time, max_time))\n    \n    # initialize stim index\n    stim_ind &lt;- 1\n    \n    # loop over POAs\n    for (poa_ind in 1:length(poas)) {\n      # loop over directions\n      for (direction_ind in 1:length(directions)) {\n        \n        # get poa and direction labels\n        poa &lt;- poas[poa_ind]\n        direction &lt;- directions[direction_ind]\n        \n        # extract modeled standard and deviant data\n        devi &lt;- itsadug::get_modelterm(model, select=stim_ind, n.grid = nval, as.data.frame = TRUE)\n        stim_ind &lt;- stim_ind+1\n        stan &lt;- itsadug::get_modelterm(model, select=stim_ind, n.grid = nval, as.data.frame = TRUE)\n        stim_ind &lt;- stim_ind+1\n        \n        # get difference for MMN\n        dat &lt;- devi %&gt;%\n          mutate(condition = \"mmn\",\n                 fit = devi$fit - stan$fit,\n                 se.fit = sqrt(devi$se.fit^2 + stan$se.fit^2))\n        \n        # subset search data\n        sdat &lt;- dat[dat$time&gt;=search_min & dat$time&lt;=search_max, ]\n        \n        # get derivative and search for peak (derivative=0) of a negativity (previous derivative value &lt; 0, which means the actual ERP waveform is decreasing before this point)\n        drv &lt;- data.frame(diff(dat$fit)/diff(dat$time))  # derivative\n        colnames(drv) &lt;- 'dYdX'\n        drv$time &lt;- rowMeans(embed(dat$time,2)) # center the X values for plotting\n        drv$dYdX.next &lt;- c(drv$dYdX[2:nrow(drv)],NA)\n        drv$dYdX.prev &lt;- c(NA,drv$dYdX[1:(nrow(drv)-1)])\n        \n        # MMN peak: going down (&lt;0) then going up (&gt;0)\n        drv$local_peak &lt;- ((drv$dYdX &lt; 0 & drv$dYdX.next &gt; 0) | (drv$dYdX.next &gt; 0 & drv$dYdX == 0 & drv$dYdX.prev &lt; 0))\n        \n        # if at least one local peak in the search time window\n        if (sum(drv[drv$time&gt;=search_min & drv$time&lt;=search_max, ]$local_peak, na.rm=TRUE) &gt;= 1) {\n          hasPeak = TRUE\n          # get all peak times\n          all_peak_times &lt;- drv[which(drv$local_peak & drv$time&gt;=search_min & drv$time&lt;=search_max), \"time\"]\n          # initialize peak height with some larger value\n          peak_height &lt;- Inf\n          # loop over local peak times\n          for (peak_ind in 1:length(all_peak_times)) {\n            # get the two fitted data points centering the local peak\n            peakdat = dat[dat$time &gt;= floor(all_peak_times[peak_ind]) & dat$time &lt;= ceiling(all_peak_times[peak_ind]), ]\n            # if the current height is smaller than the original peak height\n            if ( min(peakdat$fit) &lt; peak_height) {\n              # update peak height\n              peak_height &lt;- min(peakdat$fit)\n              # update peak time\n              peak_time &lt;- all_peak_times[peak_ind]\n              # update se\n              peak_se &lt;- peakdat[which.min(peakdat$fit),]$se.fit\n              # update NMP (original code doesn't have 1.96 factor)\n              NMP &lt;- peak_height / (1.96*peak_se) # relative peak measure (if &lt; 1 then 95%CI overlaps with 0 at point of peak)\n            }\n          }\n        } else { # if no local peak\n          hasPeak &lt;- FALSE\n          # get general peak in search span\n          subdat &lt;- dat[dat$time&gt;=search_min & dat$time&lt;=search_max, ] # subset data\n          # find peak\n          peak_height &lt;- min(subdat$fit)\n          peak_index &lt;- which.min(subdat$fit)\n          # get time\n          peak_time &lt;- subdat[peak_index, \"time\"] # first time value with peak value\n          peak_se &lt;- subdat[peak_index, ]$se.fit\n          NMP &lt;- peak_height / (1.96*peak_se)\n        }\n        \n        # if we are looking for a valley but the value is positive, then no correct positivity/negativity\n        if (peak_height &gt;= 0) {\n          peak_height &lt;- NA\n          peak_time &lt;- NA\n          peak_se &lt;- NA\n          NMP &lt;- NA\n        }\n        \n        # if peak time is the first point, there is so no real minimum\n        if (peak_time == min(sdat$time)) {\n          peak_height &lt;- NA\n          peak_time &lt;- NA\n          peak_se &lt;- NA\n          NMP &lt;- NA\n        }\n        \n        # get average of the GAM smooth in the tradition erp time window\n        gam_erp = mean(dat[dat$time&gt;=trad_min & dat$time&lt;=trad_max, ]$fit)\n        \n        # get area and fractional are latency\n        if (is.na(peak_time)) {\n          area &lt;- NA\n          half_area_latency &lt;- NA\n        } else {\n          # initialize are\n          area &lt;- 0\n          start = round(peak_time) # start time to take integral from\n          # firsttime = search_min\n          # lasttime = search_max\n          # area to the right from the peak\n          for (i in start:search_max) {\n            val = sdat[sdat$time == i,]$fit\n            \n            # if derivative &lt;=0\n            if (val &lt;= 0) {\n              area = area + abs(val)\n            } else { # end of peak, so stop going in this direction\n              break\n            }\n            lasttime = i\n          }\n          # area to the left from the peak\n          beforestart = start-1\n          if (beforestart &gt;= search_min) {\n            for (j in beforestart:search_min) { # to the left from the peak\n              val = sdat[sdat$time == j,]$fit\n              # if derivative &lt;=0\n              if (val &lt;= 0) {\n                area = area + abs(val)\n              } else { # end of peak, so stop going in this direction\n                break\n              }\n              firsttime = j\n            }\n          }\n          \n          # get half area latency\n          halfarea &lt;- 0\n          for (k in firsttime:lasttime) {\n            val = sdat[sdat$time == k, ]$fit\n            halfarea = halfarea + abs(val)\n            if (halfarea &gt;= 0.5 * area) {\n              half_area_latency = k\n              break\n            }\n          }\n        } # get area and fractional are latency end\n        \n        # get measures for the current condition\n        tmp_row &lt;- data.frame(participant, poa, direction, hasPeak, area, peak_height, peak_se, NMP, peak_time, half_area_latency, gam_erp)\n        # add to the extracted data\n        df_gam &lt;- rbind(df_gam, tmp_row)\n        \n        #%%%%% plot for each condition start %%%%%\n        # get data for plotting\n        dat$fit.plus.se = dat$fit + dat$se.fit\n        dat$fit.minus.se = dat$fit - dat$se.fit\n        \n        # raw ERP data for plotting\n        erp_df &lt;- model$model %&gt;%\n          group_by(condition, time) %&gt;%\n          summarize(mean_uV = mean(uV)) %&gt;%\n          pivot_wider(names_from = condition, values_from = mean_uV) %&gt;%\n          mutate(\n            uV_diff = .data[[paste0(poas[poa_ind], \"/\", directions[direction_ind], \"/\", \"devi\")]] - .data[[paste0(poas[poa_ind], \"/\", directions[direction_ind], \"/\", \"stan\")]]\n          )\n        \n        # df for shades\n        segments_df &lt;- sdat %&gt;%\n          filter(time&gt;=firsttime & time&lt;=lasttime) %&gt;%\n          mutate(x = time, xend = time,\n                 y = 0, yend = fit)\n        \n        # plotting\n        fig &lt;- ggplot(dat, aes(x = time, y = fit)) +\n          # ribbon\n          geom_ribbon(aes(ymin = fit - se.fit, ymax = fit + se.fit), fill = \"skyblue\", alpha = 0.3) +\n          # gam modelled mmn\n          geom_line(color = \"blue\", linewidth = 1) +\n          # shade the search window\n          annotate(\"rect\", xmin = search_min, xmax = search_max, ymin = -Inf, ymax = Inf, fill = \"red\", alpha = 0.1) +\n          # add horizontal and vertical zero lines\n          geom_vline(xintercept = 0, linetype = \"solid\", alpha = 0.2) +\n          geom_hline(yintercept = 0, linetype = \"solid\", alpha = 0.2) +\n          # mark peak time\n          geom_vline(xintercept=peak_time, linetype = \"dashed\", linewidth=1) +\n          # mark fractional area latency\n          geom_vline(xintercept=half_area_latency, linetype = \"dotted\", linewidth=1) +\n          # add derivative (scaled for visualization)\n          geom_line(data = drv, aes(x = time, y = dYdX*100), color = 'red', linetype = \"dashed\") +\n          # add shades to the modeled are\n          geom_segment(data = segments_df, aes(x=x, xend=xend, y=y, yend=yend), color = \"blue\", alpha = 0.1) +\n          # raw difference erp\n          geom_line(data = erp_df, aes(time, uV_diff), linetype = \"dotted\", color=\"black\", linewidth=0.5) +\n          theme_bw() +\n          labs(title = paste0(participant, \"_\", poa, \"_\", direction, \"_identityMMN\"),\n               subtitle = paste0(\"NMP = \",round(NMP,digits=2),\" (uV: \",round(peak_height,digits=2),\", SE: \", round(peak_se,digits=2), \")\"),\n               x = \"Time (ms)\", y = \"uV\")\n        # print(fig)\n        ggsave(plot = fig, width = 8, height = 5, units = \"in\", dpi = 300, filename = paste0(\"~/OneDrive - University of Toronto/Projects/Laryngeal/figures/gam/GAM_iMMN_\", participant, \"_\", poa, \"_\", direction, \".png\"))\n        #### plot for each condition end ####\n        \n      } # loop over directions end\n    } # loop over POAs end\n  } # loop over participants end\n} # loop over groups end\n\n\n\n5 Brain-behavioral correlation",
    "crumbs": [
      "GAM modeling"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html",
    "href": "laryngeal_preprocessing_gam.html",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "",
    "text": "#### libraries ####\nimport os\nimport mne\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport pandas as pd\n\n\nfrom scipy import signal\nfrom scipy.io import wavfile\n\n# from pybv import write_brainvision\nfrom pyprep.prep_pipeline import PrepPipeline\nfrom mne_icalabel import label_components",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters",
    "href": "laryngeal_preprocessing_gam.html#parameters",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "2.1 parameters",
    "text": "2.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/../raw data/eeg_brainvision/'\noutput_dir = os.getcwd() + '/../preprocessed/1_trigger_lag_corrected/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n\n# create a dictionary for blocks and block markers\nblock_dict = {\n    'dorsal_highStan_lowDevi': 1000, # standard: dorsal_high; deviant: dorsal_low\n    'dorsal_lowStan_highDevi': 2000, # standard: dorsal_low; deviant: dorsal_high\n    'glottal_highStan_lowDevi': 3000, # standard: glottal_high; deviant: glottal_low\n    'glottal_lowStan_highDevi': 4000, # standard: glottal_low; deviant: glottal_high\n}\n\n# exclude participants with missing data\nexclude_ppts = [\n    '0027', # vot, incomplete eeg recording due to wrong sound library setting\n    '0028', # f0, missing block 3 and 4\n    '0030', # missing block 3 and 4\n    '0034', # missing block 3\n    '0040', # f0, missing block 3 and 4\n    '0046', # f0, missing block 2,3,4\n]\n\n\n# trigger searching window (actual trigger time based on audio - trigger time in the data)\nt_left = -0.01\nt_right = 0.4 # long latency due to using pygame sound library\n\n\n#######################################################\n#### create a dictionary for trigger codes and descriptions ####\ndf = pd.read_csv(\"vot_mapping.txt\", delimiter='\\t')\nVOT_mapping = dict(zip(df['code'], df['description']))  # key = 'id', value = 'name'\n\ndf = pd.read_csv(\"f0_mapping.txt\", delimiter='\\t')\nF0_mapping = dict(zip(df['code'], df['description']))  # key = 'id', value = 'name'\n#######################################################\n\n# initialize a dictionary to save bad stims\nall_bad_stim_dict = {}\n\n# read in eeg data files\nall_files = os.listdir(input_dir)\n\n#### for each file, create a dictionary to store each block and the indices of trials of that block  ####\nfor file in all_files:\n    if file.endswith('.vhdr') and (file.split('_')[1] not in exclude_ppts) and (file.split('.')[0]+ '_corr.fif' not in os.listdir(output_dir)):\n\n        # read in vhdr files\n        raw = mne.io.read_raw_brainvision(input_dir + file, preload = True)\n\n        # If the aux channel is not named 'StimTrak', change the channel name to 'StimTrak'\n        if raw.info['ch_names'][31] != 'StimTrak':\n            # change the channel name\n            raw.rename_channels({raw.info['ch_names'][31]: 'StimTrak'})\n            # specify the audio channel\n            raw.set_channel_types({'StimTrak':'misc'})\n        \n        # extract the info of experiment version\n        exp_ver = file.split('.')[0].split('_')[2]\n\n        # extract sampling rate\n        eeg_sfreq = raw.info['sfreq']\n        \n        #############################################################################################\n        #### create a dictionary for each stim's standard version and deviant version of markers ####\n        \n        # intialize a dictionary for triggers\n        trigger_dict = {}\n        \n        # read in trigger_codes file, and create a standard trigger code and a deviant triggre code for each stimulus\n        with open('../experiment programs/'+exp_ver+'_eeg/trigger_codes.txt','r') as tf:\n            for line in tf:\n                # read in the current line\n                line = line.replace('\\n','')\n                # separate fileNames and triggerMarker\n                label, marker = line.split('\\t')\n                # convert trigger markers to integer\n                marker = int(marker)\n                # create label for stims used as standards\n                trigger_dict[marker] = label + '-s'\n                # add 100 for deviant marker\n                marker_deviant = marker + 100\n                # create label for stims used as deviants\n                trigger_dict[marker_deviant] = label + '-d'\n        #############################################################################################\n\n        ########################\n        #### get audio data ####\n        \n        # initialize dictionaries\n        audio = {}\n        lengths = {}\n        \n        # del trigger_dict[99999]\n        # for each trigger code and label in the trigger dictionary\n        for marker,label in trigger_dict.items():\n            # extract file name, up to the -2 character of the label\n            file_name = label[:-2]\n            # if not already in audio dictionary, get the info of the audio file\n            if file_name not in audio:\n                # get sample rate and data of the audio file\n                sampleRate, data = wavfile.read('../experiment programs/'+exp_ver+'_eeg/stimuli/{}.wav'.format(file_name))\n                # calculate sound file length\n                lengths[file_name] = len(data)/sampleRate\n                # reduce the sampling rate of the audio file by the factor of int(sampleRate/eeg_sfreq)\n                data_downsampled = signal.decimate(data, int(sampleRate/eeg_sfreq), ftype='fir')\n                # add info the audio dictionary\n                audio[file_name] = data_downsampled\n        ########################\n\n        ##################################################################\n        #### recode trigger marker to reflect condition and stim info ####\n\n        # for each stimulus, mark the block info\n        events_from_annot, event_dict = mne.events_from_annotations(raw, verbose='WARNING')\n        # remove the New Segment marker (which has a trigger value of 99999) and pause marker (222) and continue marker (223)\n        events_from_annot = events_from_annot[events_from_annot[:, 2] &lt; 200]\n        \n        # initialize the train for each standards+deviant sequence\n        train = np.array([]).astype(int)\n        \n        all_block = {}\n        isStandard = True # whether the standard in a train has been noted\n        \n        # loop over each trigger\n        for i in range(events_from_annot.shape[0]):\n            # add the current token to the train\n            train = np.append(train,i)\n\n            # if the trigger value is smaller than 100 and the standard toggle is true\n            if (events_from_annot[i,2]&lt;100) & isStandard:\n                # split the file name\n                trigger_splitted = trigger_dict[events_from_annot[i,2]].split('_')\n                # get the standard category name\n                block = trigger_splitted[1] + '_' + trigger_splitted[2] + 'Stan'\n                # toggle standard\n                isStandard = False\n\n            # if the trigger value is over 100\n            elif events_from_annot[i,2]&gt;100:\n                # split the file name\n                trigger_splitted = trigger_dict[events_from_annot[i,2]].split('_')\n                # append the deviant stim category \n                block = block + '_' + trigger_splitted[2] + 'Devi'\n\n                # if the block is not present in all block\n                if block not in all_block:\n                    # add the new block and the token idx\n                    all_block[block] = train[2:] # [2:] to exclude the first 2 standards\n                else:\n                    # add the token idx to the existing block\n                    all_block[block] = np.concatenate([all_block[block],train[2:]],axis = None)\n\n                # reset train\n                train = np.array([]).astype(int)\n\n                # toggle standard note\n                isStandard = True\n        \n        # loop over each block and its trigger index\n        for k,v in all_block.items():\n            # recode the trigger value to reflect block and stim category\n            events_from_annot[tuple(v),2] += block_dict[k]\n\n        # exclude the first 2 standards\n        events_from_annot = events_from_annot[events_from_annot[:,2]&gt;1000]\n        ##################################################################\n\n        \n        ######################################################\n        #### cross correction to detect the trigger delay ####\n        \n        # initialize delay list\n        delays = np.array([])\n        # initialize bad stim list\n        bad_stim = []\n        corr_results = []\n        \n        # loop over each event\n        for i in range(events_from_annot.shape[0]):\n        \n            # get current event info [time, duration, annotation]\n            event = events_from_annot[i]\n            # get the onset latency (s) of the event\n            time = event[0]/eeg_sfreq\n            # get the file name of the event\n            name = trigger_dict[event[2]%100].split('-')[0]\n            # get the data from the sound channel\n            audio_eeg = raw.get_data(\n                picks = ['StimTrak'],\n                tmin = time + t_left,\n                tmax = time + lengths[name] + t_right,\n            )[0]\n            # actual stimulus\n            audio_stim = audio[name]\n            # Z-score normalization (subtract mean, divide by std)\n            audio_eeg = (audio_eeg - np.mean(audio_eeg)) / np.std(audio_eeg)\n            audio_stim = (audio_stim - np.mean(audio_stim)) / np.std(audio_stim)\n            \n            # cross-correlation\n            corr = signal.correlate(audio_eeg, audio_stim, mode='full')\n            # Normalize cross-correlation\n            corr = corr / (np.linalg.norm(audio_eeg) * np.linalg.norm(audio_stim))\n            # Find peak correlation value\n            max_corr = np.max(corr)            \n            \n            # if the maximum correction (sum of products) is less than a threshold (empirical threshold, 0.5 is generally good)\n            if max_corr &lt; 0.5:\n                # mark the stim bad\n                bad_stim.append(i)\n\n            # append the maximum correlation\n            corr_results.append(max_corr)\n\n            \n            # the lags for cross-correlation\n            lags = signal.correlation_lags(\n                audio_eeg.size,\n                audio_stim.size,\n                mode=\"full\")\n            # get the lag of the maximum cross-correlation\n            lag = lags[np.argmax(corr)] + t_left*eeg_sfreq\n            \n            # save the lag for non-starting events\n            delays = np.append(delays,lag)\n        ######################################################\n\n        ##################################################################################################\n        #### plot the wave from the stim track and the eeg channel of the token with the minimum corr ####\n        \n        min_corr = np.argmin(corr_results)\n        # get current event info [time, duration, annotation]\n        event = events_from_annot[min_corr]\n        # get the onset latency (s) of the event\n        time = event[0]/eeg_sfreq\n        # get the file name of the event\n        name = trigger_dict[event[2]%100].split('-')[0]\n        # get the data from the sound channel\n        audio_eeg = raw.get_data(\n            picks = ['StimTrak'],\n            tmin = time + t_left,\n            tmax = time + lengths[name] + t_right,\n        )[0]\n        # actual stimulus\n        audio_stim = audio[name]\n        # Z-score normalization (subtract mean, divide by std)\n        audio_eeg = (audio_eeg - np.mean(audio_eeg)) / np.std(audio_eeg)\n        audio_stim = (audio_stim - np.mean(audio_stim)) / np.std(audio_stim)\n        # plot\n        fig, ax = plt.subplots()\n        ax.plot(audio_eeg, label = 'StimTrak', alpha = 0.6)\n        ax.plot(audio_stim, label = 'wave', alpha = 0.6)\n        ax.set_title(file)\n        ax.legend()\n        fig.savefig(output_dir + file.split('.')[0] + \"_minCor.png\", dpi=300, bbox_inches='tight')\n        ##################################################################################################\n\n        \n        ################################################\n        #### correct for trigger lag and save files ####\n\n        # add number of bad stim info\n        all_bad_stim_dict[file] = len(bad_stim)\n        \n        # remove items associated with bad stims from the event list\n        events_from_annot = np.delete(events_from_annot, bad_stim, 0)\n        \n        # remove items associated with bad stims from the delay list\n        delays = np.delete(delays, bad_stim, 0)\n        \n        # add delay back to the onset latency of each event\n        events_from_annot[:,0] = events_from_annot[:,0] + delays\n        \n        # convert individual event marker to conditions\n        # events_from_annot[:,2] = events_from_annot[:,2] - events_from_annot[:,2]%100\n        \n        # create annotations\n        annot_from_events = mne.annotations_from_events(\n            events = events_from_annot,\n            event_desc = eval(exp_ver + '_mapping'),\n            sfreq = eeg_sfreq\n        )\n        \n        # set annotations\n        raw.set_annotations(annot_from_events)\n        \n        # drop the audio channel in data\n        raw.drop_channels(['StimTrak'])\n        \n        # save as a file-into-file data\n        raw.save(output_dir + file.split('.')[0]+ '_corr.fif')\n\n        # save lag data\n        np.savetxt(output_dir + file.replace('.vhdr', '_delays.txt'), delays, fmt='%i')\n        ################################################\n\n\n# save the number of bad stims of all participant\nwith open(output_dir + 'bad_stim.txt', 'a') as f:\n    for key, value in all_bad_stim_dict.items():\n        if value &gt; 0:\n            f.write(key + '\\t' + str(value) + '\\n')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters-1",
    "href": "laryngeal_preprocessing_gam.html#parameters-1",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "3.1 parameters",
    "text": "3.1 parameters\n\n# set directory\ninput_dir = os.getcwd() + '/../preprocessed/1_trigger_lag_corrected/'\noutput_dir = os.getcwd() + '/../preprocessed/2_bad_channel_corrected/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# filter cutoff frequencies (low/high)\nf_low = 1\nf_high = 100\n\n# resampling frequency\nf_res = 250\n\n# line frequency\nline_freq = 60\n\n# preprocessing parameters\nprep_params = {\n    \"ref_chs\": 'eeg',\n    \"reref_chs\": 'eeg', # average re-reference\n    \"line_freqs\": np.arange(line_freq, f_res/2, line_freq),\n}\n\n# create a montage file for the pipeline\nmontage = mne.channels.make_standard_montage(\"standard_1020\")\n\n# interpolation method\n# method=dict(eeg=\"spline\")\n\n\n#####################################################\n#### Preprocessing (filtering, resampling, bad channel detection/interpoloation, re-reference) ####\n#####################################################\n\n# get all file namesin the folder\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n\nfor file in all_input:\n    if file.endswith(\"corr.fif\") and (file.split('.')[0]+ '_prep.fif' not in all_output):\n        \n        # read in file\n        raw = mne.io.read_raw_fif(input_dir + file, preload=True)\n\n        # set channel type\n        raw.set_channel_types({'Fp1':'eog', 'Fp2':'eog'})\n\n        # filter\n        raw.filter(l_freq = f_low, h_freq = f_high)\n        \n        #### cut off the beginning and ending part ####\n        \n        # get the onset of the first and the last event ####\n        events_from_annot, event_dict = mne.events_from_annotations(raw, verbose='WARNING')\n\n        # define the beginning time (in seconds)\n        crop_start = events_from_annot[0][0]/raw.info['sfreq'] - 10\n\n        # define the ending time (in seconds)\n        crop_end = events_from_annot[-1][0]/raw.info['sfreq'] + 10\n\n        # crop the data\n        raw.crop(\n            tmin=max(crop_start, raw.times[0]), \n            tmax=min(crop_end, raw.times[-1])\n        )\n        \n        # resample\n        raw.resample(sfreq = f_res)\n\n        # read in channel location info\n        raw.set_montage(montage)\n        \n        ####  Use PrePipeline to preprocess ####\n        '''\n        1. detect and interpolate bad channels\n        2. remove line noise\n        3. re-reference\n        '''\n\n        # apply pyprep\n        prep = PrepPipeline(raw, prep_params, montage, random_state=42)\n        prep.fit()\n        \n        # export a txt file for the interpolated channel info\n        with open(output_dir + 'bad_channel.txt', 'a+') as f:\n            _ =f.write(\n                file + ':\\n' +\n                \"- Bad channels original: {}\".format(prep.noisy_channels_original[\"bad_all\"]) + '\\n' +\n                \"- Bad channels after robust average reference: {}\".format(prep.interpolated_channels) + '\\n' +\n                \"- Bad channels after interpolation: {}\".format(prep.still_noisy_channels) + '\\n'\n            )\n\n        # save the pyprep preprocessed data\n        raw = prep.raw\n\n        # add back the reference channel\n        raw = mne.add_reference_channels(raw,'TP9')\n\n        # add the channel loc info (for the newly added reference channel)\n        raw.set_montage(montage)\n        \n        # save\n        raw.save(output_dir + file.split('.')[0]+ '_prep.fif')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters-2",
    "href": "laryngeal_preprocessing_gam.html#parameters-2",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "4.1 parameters",
    "text": "4.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/../preprocessed/2_bad_channel_corrected/'\noutput_dir = os.getcwd() + '/../preprocessed/3_ica/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# up to which IC you want to consider\nic_upto = 15\n# ic_upto = 99\n\n\n# get all file names in the folder\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n# initialize a dictionary for files \nfor file in all_input:\n    if file.endswith(\"prep.fif\") and (file.split('.')[0]+ '_ica.fif' not in all_output): \n\n        # read in file\n        raw = mne.io.read_raw_fif(input_dir + file, preload=True)\n        \n        # make a filtered file copy ICA. It works better on signals with 1 Hz high-pass filtered and 100 Hz low-pass filtered\n        raw_filt = raw.copy().filter(l_freq = 1, h_freq = 100)\n    \n        # apply a common average referencing, to comply with the ICLabel requirements\n        raw_filt.set_eeg_reference(\"average\")\n        \n        # initialize ica parameters\n        ica = mne.preprocessing.ICA(\n            # n_components=0.999999,\n            max_iter='auto', # n-1\n            # use ‘extended infomax’ method for fitting the ICA, to comply with the ICLabel requirements\n            method = 'infomax', \n            fit_params = dict(extended=True),\n            random_state = 42,\n        )\n    \n        # get ica solution\n        ica.fit(raw_filt, picks = ['eeg'])\n\n        # save ica solutions\n        ica.save(output_dir + file.split('.')[0]+ '_icaSolution.fif')\n        \n        # use ICLabel for automatic IC labeling\n        ic_labels = label_components(raw_filt, ica, method=\"iclabel\")\n\n        # save\n        with open(output_dir + file.split('.')[0]+ '_icLabels.pickle', 'wb') as f:\n            pickle.dump(ic_labels, f)\n        \n        # exclude bad IC\n        labels = ic_labels[\"labels\"]\n        exclude_idx = [\n            idx for idx, label in enumerate(labels) if idx&lt;ic_upto and label not in [\"brain\", \"other\"]\n        ]\n    \n        # ica.apply() changes the Raw object in-place\n        ica.apply(raw, exclude=exclude_idx)\n    \n        # record the bad ICs in bad_ICs.txt\n        with open(output_dir + '/bad_ICs.txt', 'a+') as f:\n            _ = f.write(file + '\\t' + str(exclude_idx) + '\\n')\n    \n        # save data\n        raw.save(output_dir + file.split('.')[0]+ '_ica.fif')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters-3",
    "href": "laryngeal_preprocessing_gam.html#parameters-3",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "5.1 parameters",
    "text": "5.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/../preprocessed/3_ica/'\noutput_dir = os.getcwd() + '/../preprocessed/4_erp_epochs/' # for ERP \n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# epoch window: \nerp_t_start = -0.2; erp_t_end = 0.8\nbaseline = (-0.2, 0)\n\n# criteria to reject epoch\n# reject_criteria = dict(eeg = 100e-6)       # 100 µV\n# reject_criteria = dict(eeg = 150e-6)       # 150 µV\nreject_criteria = dict(eeg=200e-6)       # 200 µV\n\n\n# initialize a list for participants with too many bad trials\ntoo_many_bad_trial_participants = []\n\n# get file names\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n\n# re-reference, then epoch\nfor file in all_input:\n    \n    if file.endswith(\"ica.fif\") and (file.split('_')[2] + '_' + file.split('_')[1] + '_epo.fif' not in all_output):\n        \n        # read in data\n        raw = mne.io.read_raw_fif(input_dir + file, preload = True)\n        \n        # average-mastoids re-reference\n        raw.set_eeg_reference(ref_channels = ['TP9', 'TP10'])\n        \n        #### this is for source calculation ####\n        # filter the data, optional\n        # raw = raw.filter(l_freq=None, h_freq=30) \n\n        # sphere = mne.make_sphere_model('auto', 'auto', raw.info)\n        # src = mne.setup_volume_source_space(sphere=sphere, exclude=30., pos=15.)\n        # forward = mne.make_forward_solution(raw.info, trans=None, src=src, bem=sphere)\n        # raw = raw.set_eeg_reference('REST', forward=forward)\n        ########################################\n\n        # pick EEG channels\n        # picks = mne.pick_types(raw.info, eeg = True)\n        \n        # get event info for segmentation\n        events_from_annot, event_dict = mne.events_from_annotations(raw, verbose='WARNING')\n        \n        # segmentation for ERP\n        epochs = mne.Epochs(\n            raw,\n            events = events_from_annot, event_id = event_dict,\n            tmin = erp_t_start, tmax = erp_t_end,\n            # apply baseline correction\n            baseline = baseline,\n            # remove epochs that meet the rejection criteria\n            reject = reject_criteria,\n            preload = True,\n        )\n\n        ##########################################################\n        #### remove 0-trial events, and log segmentation info ####\n\n        ppt = file.split('_')[2] + '_' + file.split('_')[1]\n        \n        for k, v in event_dict.items():\n            \n            # good trial count\n            trial_count = len(epochs[k])\n            \n            # remove 0 trial event\n            if trial_count==0:\n                del epochs.event_id[k]\n                \n            # good trial rate\n            goodTrial_rate = round( trial_count/sum(events_from_annot[:,2]==v), 2 )\n            \n            # record epoch summary\n            with open(output_dir + 'epoch_summary.txt', 'a+') as f:\n                _ =f.write(ppt + '\\t' + k + '\\t' + str(trial_count) + '\\t' + str(goodTrial_rate) + '\\n')\n\n            # mark a participant bad if any condition has fewer than 1/2 trials\n            if ( goodTrial_rate &lt; 0.5 ):\n                # mark the participant file as bad\n                if ppt not in too_many_bad_trial_participants:\n                    too_many_bad_trial_participants.append(ppt)\n        ##########################################################\n\n        # save single participant file\n        epochs.save(output_dir + ppt + '_epo.fif', overwrite=True)\n\n\n# export the record of bad participants\nwith open(output_dir + 'too_many_bad_trial_participants.txt', 'w') as f:\n    for item in too_many_bad_trial_participants:\n        f.write(item + '\\n')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters-4",
    "href": "laryngeal_preprocessing_gam.html#parameters-4",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "6.1 parameters",
    "text": "6.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/../preprocessed/4_erp_epochs/'\noutput_dir = os.getcwd() + '/../preprocessed/5_averaged/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n\n#### get ERP ####\n\n# get file names\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n# initialize a dictionary to store data\nall_evokeds = {}\n\n# for each file\nfor file in all_input:\n    \n    if file.endswith(\"_epo.fif\"):\n        \n        # extract participant number\n        ppt = file.split('_')[0] + '_' + file.split('_')[1]\n        \n        # read in data\n        epochs = mne.read_epochs(input_dir + file, preload = True)\n        \n        # average | get ERP for each condition\n        evoked = epochs.average(by_event_type=True)\n\n        # initialize dictionary for single-participant ERP\n        all_evokeds[ppt] = {}\n\n        # add key for each condition for analysis\n        for cond in evoked:\n            # append the evoked data to the dictioncary of evoked data\n            all_evokeds[ppt][cond.comment] = cond\n\n# Saving the ERP data:\nwith open(output_dir + '/all_evokeds.pkl', 'wb') as f:\n    pickle.dump(all_evokeds, f)\ndel all_evokeds",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  }
]