[
  {
    "objectID": "laryngeal_analysis_gam.html",
    "href": "laryngeal_analysis_gam.html",
    "title": "Laryngeal analysis (gam)",
    "section": "",
    "text": "The analysis goal is to extract eeg GAM measure and correlate GAM-based measure with behavioral data.\nThe model to fit each individual’s data is: bam(uV ~ s(time, condition, bs = ‘fs’, m = 1) + s(time, stim, bs = ‘fs’, m = 1), data = df, discrete = TRUE)\n\ncondition is the interaction term of type (standard vs. deviant) x poa (dorsal vs. glottal)",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "laryngeal_analysis_gam.html#single-participant-illustration",
    "href": "laryngeal_analysis_gam.html#single-participant-illustration",
    "title": "Laryngeal analysis (gam)",
    "section": "4.1 Single-participant illustration",
    "text": "4.1 Single-participant illustration\nThe model to fit each individual’s data is:\nbam(uV ~ s(time, condition, bs = ‘fs’, m = 1) + s(time, stim, bs = ‘fs’, m = 1), discrete = TRUE)\n\n# parameters ####\n\n# search window \nsearch_min = 0; search_max = 800;\n# pre-defined window (this can be from permutation test)\ntrad_min &lt;- 150; trad_max &lt;- 300\n\n# single participant id\nppt &lt;- \"F0_0001\"\n\n# prepare single participant data ####\ndf_one &lt;- df %&gt;%\n  filter(participant == ppt) %&gt;%\n  droplevels() %&gt;%\n  # logical vector\n  mutate(isDeviant = ifelse(stim_role==\"devi\", 1, 0)) # make it binary to model the difference directly\n\n# modeling #### (ignore direction)\nmodel &lt;- bam(\n  uV ~ \n    # poa * stim_role +                # Fixed effects\n    s(time, by = condition) +  # Smooth for each POA x stim_role\n    s(time, item, bs = \"fs\", m = 1),             # Random smooth by item\n  data = df_one,\n  discrete = TRUE  # Use this for large data for speed\n)\n\n# summary(model)\n\n\n# get peak height, peak time, and NMP ####\n\n# get values and standard error for every individual time point\nmin_time &lt;- min(model$model[, \"time\"])\nmax_time &lt;- max(model$model[, \"time\"])\nnval = length(seq(min_time, max_time))\n\nfit_dorsal_devi &lt;- itsadug::get_modelterm(model, select=1, n.grid = nval, as.data.frame = TRUE)\nfit_dorsal_stan &lt;- itsadug::get_modelterm(model, select=2, n.grid = nval, as.data.frame = TRUE)\n# dorsal MMN\ndat &lt;- fit_dorsal_devi %&gt;%\n  mutate(condition = \"mmn\",\n         fit = fit_dorsal_devi$fit - fit_dorsal_stan$fit,\n         se.fit = sqrt(fit_dorsal_devi$se.fit^2 + fit_dorsal_stan$se.fit^2))\n  \n\n# step 2: find derivative and search for peak (derivative=0) of a negativity (previous derivative value &lt; 0, which means the actual ERP waveform is decreasing before this point)\n\ndrv &lt;- data.frame(diff(dat$fit)/diff(dat$time))  # the derivative of the function\ncolnames(drv) &lt;- 'dYdX'\ndrv$time &lt;- rowMeans(embed(dat$time,2)) # center the X values for plotting\ndrv$dYdX.next &lt;- c(drv$dYdX[2:nrow(drv)],NA)\ndrv$dYdX.prev &lt;- c(NA,drv$dYdX[1:(nrow(drv)-1)])\n\n# MMN peak: before going down, after going up\ndrv$local_peak &lt;- ((drv$dYdX.next &gt; 0 & drv$dYdX &lt; 0) | (drv$dYdX.next &gt; 0 & drv$dYdX == 0 & drv$dYdX.prev &lt; 0))\n\n# if at least one local peak\nif (sum(drv[drv$time&gt;=search_min & drv$time&lt;=search_max, ]$local_peak, na.rm=TRUE) &gt;= 0) {\n  hasPeak = TRUE\n  # get all peak times\n  all_peak_times &lt;- drv[which(drv$local_peak & drv$time&gt;=search_min & drv$time&lt;=search_max), \"time\"]\n  # initialize peak height\n  peak_height &lt;- Inf\n  # if each local peak time\n  for (i in 1:length(all_peak_times)) {\n    # get the two fitted data points centering the local peak\n    peakdat = dat[dat$time &gt;= floor(all_peak_times[i]) & dat$time &lt;= ceiling(all_peak_times[i]), ]\n    # if the current height is smaller than the original peak height\n    if ( min(peakdat$fit) &lt; peak_height) {\n      # update peak height\n      peak_height &lt;- min(peakdat$fit)\n      # update peak time\n      peak_time &lt;- all_peak_times[i]\n      # update se\n      peak_se &lt;- peakdat[which.min(peakdat$fit),]$se.fit\n      # update NMP\n      NMP &lt;- peak_height / (1.96*peak_se) # relative peak measure (if &lt; 1 then 95%CI overlaps with 0 at point of peak)\n    }\n  }\n} else { # if no local peak\n  hasPeak &lt;- FALSE\n  # get general peak in search span\n  subdat &lt;- dat[dat$time&gt;=search_min & dat$time&lt;=search_max, ] # subset data\n  # find peak\n  peak_height &lt;- min(subdat$fit)\n  peak_index &lt;- which.min(subdat$fit)\n  # get time\n  peak_time &lt;- subdat[peak_index, \"time\"] # first time value with peak value\n  peak_se &lt;- subdat[peak_index, ]$se.fit\n  NMP &lt;- peak_height / (1.96*peak_se)\n}\n\n# if we are looking for a valley and the value is positive), then no correct positivity/negativity\nif (peak_height &gt;= 0) {\n  peak_height &lt;- NA\n  peak_time &lt;- NA\n  peak_se &lt;- NA\n  NMP &lt;- NA\n}\n\n# first time point should never be the maximum or the minimum, as this means that in case of a minimum, the first point is the lowest, and it is only increasing (so no real minimum)\n\n# subset search data\nsdat &lt;- dat[dat$time&gt;=search_min & dat$time&lt;=search_max, ]\n\nif (peak_time == min(sdat$time)) {\n  peak_height &lt;- NA\n  peak_time &lt;- NA\n  peak_se &lt;- NA\n  NMP &lt;- NA\n}\n\n# get average of the GAM smooth in the time window\ngam_dat = dat[dat$time&gt;=trad_min & dat$time&lt;=trad_max, ]\ngam_erp = mean(gam_dat$fit) # average of the fitted model in time window\n\n\n# get area and half-area latency ####\nif (is.na(peak_time)) { \n  area = NA\n  half_area_latency = NA\n} else { \n  area = 0\n  start = round(peak_time) # start time to take integral from\n  firsttime = search_min\n  lasttime = search_max\n  # area to the right from the peak\n  for (i in start:search_max) {\n    val = sdat[sdat$time == i,]$fit\n    \n    # if derivative &lt;=0\n    if (val &lt;= 0) {\n      area = area + abs(val)\n    } else { # end of peak, so stop going in this direction\n      break\n    }\n    lasttime = i\n  } \n  # area to the left from the peak\n  beforestart = start-1\n  if (beforestart &gt;= search_min) {\n    for (j in beforestart:search_min) { # to the left from the peak\n      val = sdat[sdat$time == j,]$fit\n      # if derivative &lt;=0\n      if (val &lt;= 0) {\n        area = area + abs(val)\n      } else { # end of peak, so stop going in this direction\n        break\n      }\n      firsttime = j\n    } \n  }\n  \n  # get half area latency\n  halfarea = 0\n  for (k in firsttime:lasttime) { \n    val = sdat[sdat$time == k, ]$fit\n    halfarea = halfarea + abs(val)\n    if (halfarea &gt;= 0.5 * area) {\n      half_area_latency = k\n      break\n    }\n  }\n}\n\n# save single-participant data\ntmp &lt;- data.frame(ppt, area, hasPeak, peak_height, peak_se, NMP, peak_time, half_area_latency, gam_erp)\n\n\n4.1.1 visualization\n\n## plotting\n\n# get data for plotting\ndat$fit.plus.se = dat$fit + dat$se.fit\ndat$fit.minus.se = dat$fit - dat$se.fit\n# get y-axis limits\nymax &lt;- ceiling(max(c(abs(dat$fit.minus.se), dat$fit.plus.se)))\nymin &lt;- -ymax\n# x-axis limites\nxmin = min(dat$time)\nxmax = max(dat$time)\nydmax &lt;- 4*max(c(abs(drv$dYdX)), drv$dYdX) # only use 1/4 of the vertical space to make the derivative less prominent\nydmin &lt;- -ydmax\n\n\n# raw ERP data\nerp_df &lt;- model$model %&gt;%\n  group_by(condition, time) %&gt;%\n  summarize(mean_uV = mean(uV)) %&gt;%\n  pivot_wider(names_from = condition, values_from = mean_uV) %&gt;%\n  mutate(uV_diff = dorsal_devi - dorsal_stan)\n\n# df for shades\nsegments_df &lt;- sdat[sdat$time %in% firsttime:lasttime, ] %&gt;%\n  mutate(x = time, xend = time,\n         y = 0, yend = fit)\n\n# plotting\nfig &lt;- ggplot(dat, aes(x = time, y = fit)) +\n  geom_ribbon(aes(ymin = fit - se.fit, ymax = fit + se.fit), fill = \"skyblue\", alpha = 0.3) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  annotate(\"rect\", xmin = search_min, xmax = search_max,\n           ymin = -Inf, ymax = Inf,\n           fill = \"red\", alpha = 0.1) +\n  geom_vline(xintercept = 0, linetype = \"solid\", alpha = 0.2) +\n  geom_hline(yintercept = 0, linetype = \"solid\", alpha = 0.2) +\n  # add line for peak time\n  geom_vline(xintercept=peak_time, linetype = \"dashed\", linewidth=1) +\n  geom_vline(xintercept=half_area_latency, linetype = \"dotted\", linewidth=1) +\n  # add derivative\n  geom_line(data = drv, aes(x = time, y = dYdX*100), color = 'red', linetype = \"dashed\") +\n  # add shades\n  geom_segment(data = segments_df,\n    aes(x = x, xend = xend, y = y, yend = yend),\n    color = \"blue\", alpha = 0.1) +\n  # raw erp\n  geom_line(data = erp_df, aes(time, uV_diff), linetype = \"dotted\", color=\"black\", linewidth=0.5) +\n  theme_bw() +\n  labs(\n    title = \"GAM\",\n    subtitle = paste0(\"NMP = \",round(NMP,digits=2),\" (uV: \",round(peak_height,digits=2),\", SE: \", round(peak_se,digits=2), \")\"),\n    x = \"Time (ms)\", y = \"uV\")\nprint(fig)",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "laryngeal_analysis_gam.html#all-participants",
    "href": "laryngeal_analysis_gam.html#all-participants",
    "title": "Laryngeal analysis (gam)",
    "section": "4.2 All participants",
    "text": "4.2 All participants\n\n4.2.1 Visualization of the measures",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "laryngeal_analysis_gam.html#brain-behavioral-correlation",
    "href": "laryngeal_analysis_gam.html#brain-behavioral-correlation",
    "title": "Laryngeal analysis (gam)",
    "section": "4.3 Brain-behavioral correlation",
    "text": "4.3 Brain-behavioral correlation",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html",
    "href": "laryngeal_preprocessing_gam.html",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "",
    "text": "#### libraries ####\nimport os\nimport mne\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport pandas as pd\n\n\nfrom scipy import signal\nfrom scipy.io import wavfile\n\n# from pybv import write_brainvision\nfrom pyprep.prep_pipeline import PrepPipeline\nfrom mne_icalabel import label_components",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters",
    "href": "laryngeal_preprocessing_gam.html#parameters",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "2.1 parameters",
    "text": "2.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/../raw data/eeg_brainvision/'\noutput_dir = os.getcwd() + '/../preprocessed/1_trigger_lag_corrected/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n\n# create a dictionary for blocks and block markers\nblock_dict = {\n    'dorsal_highStan_lowDevi': 1000, # standard: dorsal_high; deviant: dorsal_low\n    'dorsal_lowStan_highDevi': 2000, # standard: dorsal_low; deviant: dorsal_high\n    'glottal_highStan_lowDevi': 3000, # standard: glottal_high; deviant: glottal_low\n    'glottal_lowStan_highDevi': 4000, # standard: glottal_low; deviant: glottal_high\n}\n\n# exclude participants with missing data\nexclude_ppts = [\n    '0027', # vot, incomplete eeg recording due to wrong sound library setting\n    '0028', # f0, missing block 3 and 4\n    '0030', # missing block 3 and 4\n    '0034', # missing block 3\n    '0040', # f0, missing block 3 and 4\n    '0046', # f0, missing block 2,3,4\n]\n\n\n# trigger searching window (actual trigger time based on audio - trigger time in the data)\nt_left = -0.01\nt_right = 0.4 # long latency due to using pygame sound library\n\n\n#######################################################\n#### create a dictionary for trigger codes and descriptions ####\ndf = pd.read_csv(\"vot_mapping.txt\", delimiter='\\t')\nVOT_mapping = dict(zip(df['code'], df['description']))  # key = 'id', value = 'name'\n\ndf = pd.read_csv(\"f0_mapping.txt\", delimiter='\\t')\nF0_mapping = dict(zip(df['code'], df['description']))  # key = 'id', value = 'name'\n#######################################################\n\n# initialize a dictionary to save bad stims\nall_bad_stim_dict = {}\n\n# read in eeg data files\nall_files = os.listdir(input_dir)\n\n#### for each file, create a dictionary to store each block and the indices of trials of that block  ####\nfor file in all_files:\n    if file.endswith('.vhdr') and (file.split('_')[1] not in exclude_ppts) and (file.split('.')[0]+ '_corr.fif' not in os.listdir(output_dir)):\n\n        # read in vhdr files\n        raw = mne.io.read_raw_brainvision(input_dir + file, preload = True)\n\n        # If the aux channel is not named 'StimTrak', change the channel name to 'StimTrak'\n        if raw.info['ch_names'][31] != 'StimTrak':\n            # change the channel name\n            raw.rename_channels({raw.info['ch_names'][31]: 'StimTrak'})\n            # specify the audio channel\n            raw.set_channel_types({'StimTrak':'misc'})\n        \n        # extract the info of experiment version\n        exp_ver = file.split('.')[0].split('_')[2]\n\n        # extract sampling rate\n        eeg_sfreq = raw.info['sfreq']\n        \n        #############################################################################################\n        #### create a dictionary for each stim's standard version and deviant version of markers ####\n        \n        # intialize a dictionary for triggers\n        trigger_dict = {}\n        \n        # read in trigger_codes file, and create a standard trigger code and a deviant triggre code for each stimulus\n        with open('../experiment programs/'+exp_ver+'_eeg/trigger_codes.txt','r') as tf:\n            for line in tf:\n                # read in the current line\n                line = line.replace('\\n','')\n                # separate fileNames and triggerMarker\n                label, marker = line.split('\\t')\n                # convert trigger markers to integer\n                marker = int(marker)\n                # create label for stims used as standards\n                trigger_dict[marker] = label + '-s'\n                # add 100 for deviant marker\n                marker_deviant = marker + 100\n                # create label for stims used as deviants\n                trigger_dict[marker_deviant] = label + '-d'\n        #############################################################################################\n\n        ########################\n        #### get audio data ####\n        \n        # initialize dictionaries\n        audio = {}\n        lengths = {}\n        \n        # del trigger_dict[99999]\n        # for each trigger code and label in the trigger dictionary\n        for marker,label in trigger_dict.items():\n            # extract file name, up to the -2 character of the label\n            file_name = label[:-2]\n            # if not already in audio dictionary, get the info of the audio file\n            if file_name not in audio:\n                # get sample rate and data of the audio file\n                sampleRate, data = wavfile.read('../experiment programs/'+exp_ver+'_eeg/stimuli/{}.wav'.format(file_name))\n                # calculate sound file length\n                lengths[file_name] = len(data)/sampleRate\n                # reduce the sampling rate of the audio file by the factor of int(sampleRate/eeg_sfreq)\n                data_downsampled = signal.decimate(data, int(sampleRate/eeg_sfreq), ftype='fir')\n                # add info the audio dictionary\n                audio[file_name] = data_downsampled\n        ########################\n\n        ##################################################################\n        #### recode trigger marker to reflect condition and stim info ####\n\n        # for each stimulus, mark the block info\n        events_from_annot, event_dict = mne.events_from_annotations(raw, verbose='WARNING')\n        # remove the New Segment marker (which has a trigger value of 99999) and pause marker (222) and continue marker (223)\n        events_from_annot = events_from_annot[events_from_annot[:, 2] &lt; 200]\n        \n        # initialize the train for each standards+deviant sequence\n        train = np.array([]).astype(int)\n        \n        all_block = {}\n        isStandard = True # whether the standard in a train has been noted\n        \n        # loop over each trigger\n        for i in range(events_from_annot.shape[0]):\n            # add the current token to the train\n            train = np.append(train,i)\n\n            # if the trigger value is smaller than 100 and the standard toggle is true\n            if (events_from_annot[i,2]&lt;100) & isStandard:\n                # split the file name\n                trigger_splitted = trigger_dict[events_from_annot[i,2]].split('_')\n                # get the standard category name\n                block = trigger_splitted[1] + '_' + trigger_splitted[2] + 'Stan'\n                # toggle standard\n                isStandard = False\n\n            # if the trigger value is over 100\n            elif events_from_annot[i,2]&gt;100:\n                # split the file name\n                trigger_splitted = trigger_dict[events_from_annot[i,2]].split('_')\n                # append the deviant stim category \n                block = block + '_' + trigger_splitted[2] + 'Devi'\n\n                # if the block is not present in all block\n                if block not in all_block:\n                    # add the new block and the token idx\n                    all_block[block] = train[2:] # [2:] to exclude the first 2 standards\n                else:\n                    # add the token idx to the existing block\n                    all_block[block] = np.concatenate([all_block[block],train[2:]],axis = None)\n\n                # reset train\n                train = np.array([]).astype(int)\n\n                # toggle standard note\n                isStandard = True\n        \n        # loop over each block and its trigger index\n        for k,v in all_block.items():\n            # recode the trigger value to reflect block and stim category\n            events_from_annot[tuple(v),2] += block_dict[k]\n\n        # exclude the first 2 standards\n        events_from_annot = events_from_annot[events_from_annot[:,2]&gt;1000]\n        ##################################################################\n\n        \n        ######################################################\n        #### cross correction to detect the trigger delay ####\n        \n        # initialize delay list\n        delays = np.array([])\n        # initialize bad stim list\n        bad_stim = []\n        corr_results = []\n        \n        # loop over each event\n        for i in range(events_from_annot.shape[0]):\n        \n            # get current event info [time, duration, annotation]\n            event = events_from_annot[i]\n            # get the onset latency (s) of the event\n            time = event[0]/eeg_sfreq\n            # get the file name of the event\n            name = trigger_dict[event[2]%100].split('-')[0]\n            # get the data from the sound channel\n            audio_eeg = raw.get_data(\n                picks = ['StimTrak'],\n                tmin = time + t_left,\n                tmax = time + lengths[name] + t_right,\n            )[0]\n            # actual stimulus\n            audio_stim = audio[name]\n            # Z-score normalization (subtract mean, divide by std)\n            audio_eeg = (audio_eeg - np.mean(audio_eeg)) / np.std(audio_eeg)\n            audio_stim = (audio_stim - np.mean(audio_stim)) / np.std(audio_stim)\n            \n            # cross-correlation\n            corr = signal.correlate(audio_eeg, audio_stim, mode='full')\n            # Normalize cross-correlation\n            corr = corr / (np.linalg.norm(audio_eeg) * np.linalg.norm(audio_stim))\n            # Find peak correlation value\n            max_corr = np.max(corr)            \n            \n            # if the maximum correction (sum of products) is less than a threshold (empirical threshold, 0.5 is generally good)\n            if max_corr &lt; 0.5:\n                # mark the stim bad\n                bad_stim.append(i)\n\n            # append the maximum correlation\n            corr_results.append(max_corr)\n\n            \n            # the lags for cross-correlation\n            lags = signal.correlation_lags(\n                audio_eeg.size,\n                audio_stim.size,\n                mode=\"full\")\n            # get the lag of the maximum cross-correlation\n            lag = lags[np.argmax(corr)] + t_left*eeg_sfreq\n            \n            # save the lag for non-starting events\n            delays = np.append(delays,lag)\n        ######################################################\n\n        ##################################################################################################\n        #### plot the wave from the stim track and the eeg channel of the token with the minimum corr ####\n        \n        min_corr = np.argmin(corr_results)\n        # get current event info [time, duration, annotation]\n        event = events_from_annot[min_corr]\n        # get the onset latency (s) of the event\n        time = event[0]/eeg_sfreq\n        # get the file name of the event\n        name = trigger_dict[event[2]%100].split('-')[0]\n        # get the data from the sound channel\n        audio_eeg = raw.get_data(\n            picks = ['StimTrak'],\n            tmin = time + t_left,\n            tmax = time + lengths[name] + t_right,\n        )[0]\n        # actual stimulus\n        audio_stim = audio[name]\n        # Z-score normalization (subtract mean, divide by std)\n        audio_eeg = (audio_eeg - np.mean(audio_eeg)) / np.std(audio_eeg)\n        audio_stim = (audio_stim - np.mean(audio_stim)) / np.std(audio_stim)\n        # plot\n        fig, ax = plt.subplots()\n        ax.plot(audio_eeg, label = 'StimTrak', alpha = 0.6)\n        ax.plot(audio_stim, label = 'wave', alpha = 0.6)\n        ax.set_title(file)\n        ax.legend()\n        fig.savefig(output_dir + file.split('.')[0] + \"_minCor.png\", dpi=300, bbox_inches='tight')\n        ##################################################################################################\n\n        \n        ################################################\n        #### correct for trigger lag and save files ####\n\n        # add number of bad stim info\n        all_bad_stim_dict[file] = len(bad_stim)\n        \n        # remove items associated with bad stims from the event list\n        events_from_annot = np.delete(events_from_annot, bad_stim, 0)\n        \n        # remove items associated with bad stims from the delay list\n        delays = np.delete(delays, bad_stim, 0)\n        \n        # add delay back to the onset latency of each event\n        events_from_annot[:,0] = events_from_annot[:,0] + delays\n        \n        # convert individual event marker to conditions\n        # events_from_annot[:,2] = events_from_annot[:,2] - events_from_annot[:,2]%100\n        \n        # create annotations\n        annot_from_events = mne.annotations_from_events(\n            events = events_from_annot,\n            event_desc = eval(exp_ver + '_mapping'),\n            sfreq = eeg_sfreq\n        )\n        \n        # set annotations\n        raw.set_annotations(annot_from_events)\n        \n        # drop the audio channel in data\n        raw.drop_channels(['StimTrak'])\n        \n        # save as a file-into-file data\n        raw.save(output_dir + file.split('.')[0]+ '_corr.fif')\n\n        # save lag data\n        np.savetxt(output_dir + file.replace('.vhdr', '_delays.txt'), delays, fmt='%i')\n        ################################################\n\n\n# save the number of bad stims of all participant\nwith open(output_dir + 'bad_stim.txt', 'a') as f:\n    for key, value in all_bad_stim_dict.items():\n        if value &gt; 0:\n            f.write(key + '\\t' + str(value) + '\\n')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters-1",
    "href": "laryngeal_preprocessing_gam.html#parameters-1",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "3.1 parameters",
    "text": "3.1 parameters\n\n# set directory\ninput_dir = os.getcwd() + '/../preprocessed/1_trigger_lag_corrected/'\noutput_dir = os.getcwd() + '/../preprocessed/2_bad_channel_corrected/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# filter cutoff frequencies (low/high)\nf_low = 1\nf_high = 100\n\n# resampling frequency\nf_res = 250\n\n# line frequency\nline_freq = 60\n\n# preprocessing parameters\nprep_params = {\n    \"ref_chs\": 'eeg',\n    \"reref_chs\": 'eeg', # average re-reference\n    \"line_freqs\": np.arange(line_freq, f_res/2, line_freq),\n}\n\n# create a montage file for the pipeline\nmontage = mne.channels.make_standard_montage(\"standard_1020\")\n\n# interpolation method\n# method=dict(eeg=\"spline\")\n\n\n#####################################################\n#### Preprocessing (filtering, resampling, bad channel detection/interpoloation, re-reference) ####\n#####################################################\n\n# get all file namesin the folder\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n\nfor file in all_input:\n    if file.endswith(\"corr.fif\") and (file.split('.')[0]+ '_prep.fif' not in all_output):\n        \n        # read in file\n        raw = mne.io.read_raw_fif(input_dir + file, preload=True)\n\n        # set channel type\n        raw.set_channel_types({'Fp1':'eog', 'Fp2':'eog'})\n\n        # filter\n        raw.filter(l_freq = f_low, h_freq = f_high)\n        \n        #### cut off the beginning and ending part ####\n        \n        # get the onset of the first and the last event ####\n        events_from_annot, event_dict = mne.events_from_annotations(raw, verbose='WARNING')\n\n        # define the beginning time (in seconds)\n        crop_start = events_from_annot[0][0]/raw.info['sfreq'] - 10\n\n        # define the ending time (in seconds)\n        crop_end = events_from_annot[-1][0]/raw.info['sfreq'] + 10\n\n        # crop the data\n        raw.crop(\n            tmin=max(crop_start, raw.times[0]), \n            tmax=min(crop_end, raw.times[-1])\n        )\n        \n        # resample\n        raw.resample(sfreq = f_res)\n\n        # read in channel location info\n        raw.set_montage(montage)\n        \n        ####  Use PrePipeline to preprocess ####\n        '''\n        1. detect and interpolate bad channels\n        2. remove line noise\n        3. re-reference\n        '''\n\n        # apply pyprep\n        prep = PrepPipeline(raw, prep_params, montage, random_state=42)\n        prep.fit()\n        \n        # export a txt file for the interpolated channel info\n        with open(output_dir + 'bad_channel.txt', 'a+') as f:\n            _ =f.write(\n                file + ':\\n' +\n                \"- Bad channels original: {}\".format(prep.noisy_channels_original[\"bad_all\"]) + '\\n' +\n                \"- Bad channels after robust average reference: {}\".format(prep.interpolated_channels) + '\\n' +\n                \"- Bad channels after interpolation: {}\".format(prep.still_noisy_channels) + '\\n'\n            )\n\n        # save the pyprep preprocessed data\n        raw = prep.raw\n\n        # add back the reference channel\n        raw = mne.add_reference_channels(raw,'TP9')\n\n        # add the channel loc info (for the newly added reference channel)\n        raw.set_montage(montage)\n        \n        # save\n        raw.save(output_dir + file.split('.')[0]+ '_prep.fif')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters-2",
    "href": "laryngeal_preprocessing_gam.html#parameters-2",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "4.1 parameters",
    "text": "4.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/../preprocessed/2_bad_channel_corrected/'\noutput_dir = os.getcwd() + '/../preprocessed/3_ica/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# up to which IC you want to consider\nic_upto = 15\n# ic_upto = 99\n\n\n# get all file names in the folder\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n# initialize a dictionary for files \nfor file in all_input:\n    if file.endswith(\"prep.fif\") and (file.split('.')[0]+ '_ica.fif' not in all_output): \n\n        # read in file\n        raw = mne.io.read_raw_fif(input_dir + file, preload=True)\n        \n        # make a filtered file copy ICA. It works better on signals with 1 Hz high-pass filtered and 100 Hz low-pass filtered\n        raw_filt = raw.copy().filter(l_freq = 1, h_freq = 100)\n    \n        # apply a common average referencing, to comply with the ICLabel requirements\n        raw_filt.set_eeg_reference(\"average\")\n        \n        # initialize ica parameters\n        ica = mne.preprocessing.ICA(\n            # n_components=0.999999,\n            max_iter='auto', # n-1\n            # use ‘extended infomax’ method for fitting the ICA, to comply with the ICLabel requirements\n            method = 'infomax', \n            fit_params = dict(extended=True),\n            random_state = 42,\n        )\n    \n        # get ica solution\n        ica.fit(raw_filt, picks = ['eeg'])\n\n        # save ica solutions\n        ica.save(output_dir + file.split('.')[0]+ '_icaSolution.fif')\n        \n        # use ICLabel for automatic IC labeling\n        ic_labels = label_components(raw_filt, ica, method=\"iclabel\")\n\n        # save\n        with open(output_dir + file.split('.')[0]+ '_icLabels.pickle', 'wb') as f:\n            pickle.dump(ic_labels, f)\n        \n        # exclude bad IC\n        labels = ic_labels[\"labels\"]\n        exclude_idx = [\n            idx for idx, label in enumerate(labels) if idx&lt;ic_upto and label not in [\"brain\", \"other\"]\n        ]\n    \n        # ica.apply() changes the Raw object in-place\n        ica.apply(raw, exclude=exclude_idx)\n    \n        # record the bad ICs in bad_ICs.txt\n        with open(output_dir + '/bad_ICs.txt', 'a+') as f:\n            _ = f.write(file + '\\t' + str(exclude_idx) + '\\n')\n    \n        # save data\n        raw.save(output_dir + file.split('.')[0]+ '_ica.fif')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters-3",
    "href": "laryngeal_preprocessing_gam.html#parameters-3",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "5.1 parameters",
    "text": "5.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/../preprocessed/3_ica/'\noutput_dir = os.getcwd() + '/../preprocessed/4_erp_epochs/' # for ERP \n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# epoch window: \nerp_t_start = -0.2; erp_t_end = 0.8\nbaseline = (-0.2, 0)\n\n# criteria to reject epoch\n# reject_criteria = dict(eeg = 100e-6)       # 100 µV\n# reject_criteria = dict(eeg = 150e-6)       # 150 µV\nreject_criteria = dict(eeg=200e-6)       # 200 µV\n\n\n# initialize a list for participants with too many bad trials\ntoo_many_bad_trial_participants = []\n\n# get file names\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n\n# re-reference, then epoch\nfor file in all_input:\n    \n    if file.endswith(\"ica.fif\") and (file.split('_')[2] + '_' + file.split('_')[1] + '_epo.fif' not in all_output):\n        \n        # read in data\n        raw = mne.io.read_raw_fif(input_dir + file, preload = True)\n        \n        # average-mastoids re-reference\n        raw.set_eeg_reference(ref_channels = ['TP9', 'TP10'])\n        \n        #### this is for source calculation ####\n        # filter the data, optional\n        # raw = raw.filter(l_freq=None, h_freq=30) \n\n        # sphere = mne.make_sphere_model('auto', 'auto', raw.info)\n        # src = mne.setup_volume_source_space(sphere=sphere, exclude=30., pos=15.)\n        # forward = mne.make_forward_solution(raw.info, trans=None, src=src, bem=sphere)\n        # raw = raw.set_eeg_reference('REST', forward=forward)\n        ########################################\n\n        # pick EEG channels\n        # picks = mne.pick_types(raw.info, eeg = True)\n        \n        # get event info for segmentation\n        events_from_annot, event_dict = mne.events_from_annotations(raw, verbose='WARNING')\n        \n        # segmentation for ERP\n        epochs = mne.Epochs(\n            raw,\n            events = events_from_annot, event_id = event_dict,\n            tmin = erp_t_start, tmax = erp_t_end,\n            # apply baseline correction\n            baseline = baseline,\n            # remove epochs that meet the rejection criteria\n            reject = reject_criteria,\n            preload = True,\n        )\n\n        ##########################################################\n        #### remove 0-trial events, and log segmentation info ####\n\n        ppt = file.split('_')[2] + '_' + file.split('_')[1]\n        \n        for k, v in event_dict.items():\n            \n            # good trial count\n            trial_count = len(epochs[k])\n            \n            # remove 0 trial event\n            if trial_count==0:\n                del epochs.event_id[k]\n                \n            # good trial rate\n            goodTrial_rate = round( trial_count/sum(events_from_annot[:,2]==v), 2 )\n            \n            # record epoch summary\n            with open(output_dir + 'epoch_summary.txt', 'a+') as f:\n                _ =f.write(ppt + '\\t' + k + '\\t' + str(trial_count) + '\\t' + str(goodTrial_rate) + '\\n')\n\n            # mark a participant bad if any condition has fewer than 1/2 trials\n            if ( goodTrial_rate &lt; 0.5 ):\n                # mark the participant file as bad\n                if ppt not in too_many_bad_trial_participants:\n                    too_many_bad_trial_participants.append(ppt)\n        ##########################################################\n\n        # save single participant file\n        epochs.save(output_dir + ppt + '_epo.fif', overwrite=True)\n\n\n# export the record of bad participants\nwith open(output_dir + 'too_many_bad_trial_participants.txt', 'w') as f:\n    for item in too_many_bad_trial_participants:\n        f.write(item + '\\n')",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  },
  {
    "objectID": "laryngeal_preprocessing_gam.html#parameters-4",
    "href": "laryngeal_preprocessing_gam.html#parameters-4",
    "title": "Laryngeal Preprocessing Pipeline",
    "section": "6.1 parameters",
    "text": "6.1 parameters\n\n# directory\ninput_dir = os.getcwd() + '/../preprocessed/4_erp_epochs/'\noutput_dir = os.getcwd() + '/../preprocessed/5_averaged/'\n# create a folder if the folder doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n\n#### get ERP ####\n\n# get file names\nall_input = os.listdir(input_dir)\nall_output = os.listdir(output_dir)\n\n# initialize a dictionary to store data\nall_evokeds = {}\n\n# for each file\nfor file in all_input:\n    \n    if file.endswith(\"_epo.fif\"):\n        \n        # extract participant number\n        ppt = file.split('_')[0] + '_' + file.split('_')[1]\n        \n        # read in data\n        epochs = mne.read_epochs(input_dir + file, preload = True)\n        \n        # average | get ERP for each condition\n        evoked = epochs.average(by_event_type=True)\n\n        # initialize dictionary for single-participant ERP\n        all_evokeds[ppt] = {}\n\n        # add key for each condition for analysis\n        for cond in evoked:\n            # append the evoked data to the dictioncary of evoked data\n            all_evokeds[ppt][cond.comment] = cond\n\n# Saving the ERP data:\nwith open(output_dir + '/all_evokeds.pkl', 'wb') as f:\n    pickle.dump(all_evokeds, f)\ndel all_evokeds",
    "crumbs": [
      "Preprocessing pipeline"
    ]
  }
]